cheb_est <- list(rep(NA, 1000))
cheb_beta_error <- rep(NA, 1000)
cheb_y_pred <- list(rep(NA, 1000))
cheb_y_error <- rep(NA, 1000)
ridge_est <- list(rep(NA, 1000))
ridge_beta_error <- rep(NA, 1000)
ridge_y_pred <- list(rep(NA, 1000))
ridge_y_error <- rep(NA, 1000)
lasso_est <- list(rep(NA, 1000))
lasso_beta_error <- rep(NA, 1000)
lasso_y_pred <- list(rep(NA, 1000))
lasso_y_error <- rep(NA, 1000)
fwd_est <- list(rep(NA, 1000))
fwd_beta_error <- rep(NA, 1000)
fwd_y_pred <- list(rep(NA, 1000))
fwd_y_error <- rep(NA, 1000)
fwd_sel <- rep(NA, 1000)
bwd_est <- list(rep(NA, 1000))
bwd_beta_error <- rep(NA, 1000)
bwd_y_pred <- list(rep(NA, 1000))
bwd_y_error <- rep(NA, 1000)
bwd_sel <- rep(NA, 1000)
# Vector con valores verdaderos de los coeficientes, utilizada en las
# regresiones OLS, LAD, Chebyshev, Ridge y Lasso
betas_real <- c(1,2,-2,1,0,0,1,0.5)
# Vector con valores verdaderos de los coeficientes (para todas las variables),
# utilizado en los modelos de Forward y Backward Regression
var_names <- c("(Intercept),x1,x2,x3,x4,x5,x1:x2,x1:x3,x2:x3,x1:x4",
"x2:x4,x3:x4,x1:x5,x2:x5,x3:x5,x4:x5,x1:x2:x3,x1:x2:x4",
"x1:x3:x4,x2:x3:x4,x1:x2:x5,x1:x3:x5,x2:x3:x5,x1:x4:x5",
"x2:x4:x5,x3:x4:x5,x1:x2:x3:x4,x1:x2:x3:x5,x1:x2:x4:x5",
"x1:x3:x4:x5,x2:x3:x4:x5,x1:x2:x3:x4:x5")
var_names <- sort(strsplit(paste(var_names, collapse = ","), ",")[[1]])
vec_coef <- c(c(1, 2, 1, 0.5), rep(0, 13), -2, rep(0, 7), 1, rep(0, 6))
betas_real_com <- as.list(setNames(vec_coef, var_names))
var_names_real <- c("(Intercept)", "x1", "x2", "x1:x2", "x1:x2:x3")
# Funcion para estimar "y", dados los coeficientes calculados y las "x_i",
# utilizada en regresión de Chebyshev
predict_y <- function(coef, x1, x2, x3, x4, x5) {
result <- coef[1] + coef[2]*x1 - coef[3]*x2 + coef[4]*x3 +
coef[5]*x4 + coef[6]*x5 + coef[7]*x1*x2 + coef[8]*x1*x2*x3
return(result)
}
# Funciones auxiliares para las regresiones Stepwise Forward/Backward
# Estimacion del modelo
stepwise_f <- function(y, x1, x2, x3, x4, x5, method) {
data_df <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3, x4 = x4, x5 = x5)
stepwise_model <- regsubsets(y ~ x1 * x2 * x3 * x4 * x5,
data = data_df,
nbest = 1,
intercept = TRUE,
method = method)
return(stepwise_model)
}
# Calculo del error en los coeficientes
beta_error_f <- function(model) {
new_vars <- var_names[!(var_names %in%
names(coef(model,
which.min(summary(model)$bic))))]
lista_betas <- c(coef(model, which.min(summary(model)$bic)),
as.list(setNames(rep(0, length(new_vars)), new_vars)))
return(lista_betas[order(names(lista_betas))])
}
# Funcion de prediccion para regsubsets
predict.regsubsets <- function(object, newdata, ...) {
form <- y ~ x1 * x2 * x3 * x4 * x5
mat <- model.matrix(form, newdata)
coefi <- coef(object, which.min(summary(object)$bic))
return(mat[, names(coefi)] %*% coefi)
}
# Seleccion de variables para regresiones Stepwise
stepwise_sel_f <- function(object) {
sel_vars <- names(coef(object, which.min(summary(object)$bic)))
return(sum(sel_vars %in% var_names_real)/5)
}
select_vector <- c(TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE)
for (i in 1:1000) {
# Generar los datos
x1 <- runif(100)
x2 <- runif(100)
x3 <- runif(100)
x4 <- runif(100)
x5 <- runif(100)
epsilon <- rnorm(100, mean = 0, sd = 0.5)
y <- 1 + 2*x1 - 2*x2 + x3 + 0*x4 + 0*x5 + 1*x1*x2 + 0.5*x1*x2*x3 + epsilon
# OLS
ols_est[[i]] <- lm(y ~ x1 + x2 + x3 + x4 + x5 + x1:x2 + x1:x2:x3)
ols_beta_error[i] <- norm(betas_real - coef(ols_est[[i]]), type = "2")
ols_y_error[i] <- norm(y - fitted(ols_est[[i]]), type = "2")
ols_sel[i] <- sum((coef(summary(ols_est[[i]]))[, "Pr(>|t|)"] < 0.05) ==
select_vector)/8
# LAD
lad_est[[i]] <- rq(y ~ x1 + x2 + x3 + x4 + x5 + x1:x2 + x1:x2:x3)
lad_beta_error[i] <- norm(betas_real - coef(lad_est[[i]]), type = "2")
lad_y_error[i] <- norm(y - fitted(lad_est[[i]]), type = "2")
# Matriz utilizada para los modelos de Chebyshev, Ridge y Lasso
mat_x <- cbind(x1, x2, x3, x4, x5, x1x2=x1*x2, x1x2x3=x1*x2*x3)
# Chebyshev
cheb_est[[i]] <- chebR(a = cbind(rep(1, 100), mat_x), b = y)
cheb_beta_error[i] <- norm(betas_real - cheb_est[[i]]$coefs, type = "2")
cheb_y_pred[[i]] <- predict_y(cheb_est[[i]]$coefs, x1, x2, x3, x4, x5)
cheb_y_error[i] <- norm(y - cheb_y_pred[[i]], type = "2")
# Ridge
optim_lambda <- cv.glmnet(x = mat_x, y = y, alpha = 0,
nfolds = 10,
standardize = FALSE)$lambda.min
ridge_est[[i]] <- glmnet(mat_x, y, alpha = 0,
lambda = optim_lambda)
ridge_beta_error[i] <- norm(betas_real - as.vector(coef(ridge_est[[i]])),
type = "2")
ridge_y_pred[[i]] <- predict(ridge_est[[i]], s = optim_lambda, newx = mat_x)
ridge_y_error[i] <- norm(y - as.vector(ridge_y_pred[[i]]), type = "2")
# Lasso
optim_lambda <- cv.glmnet(x = mat_x, y = y, alpha = 1,
nfolds = 10,
standardize = FALSE)$lambda.min
lasso_est[[i]] <- glmnet(mat_x, y, alpha = 1,
lambda = optim_lambda)
lasso_beta_error[i] <- norm(betas_real - as.vector(coef(lasso_est[[i]])),
type = "2")
lasso_y_pred[[i]] <- predict(lasso_est[[i]], s = optim_lambda, newx = mat_x)
lasso_y_error[i] <- norm(y - as.vector(lasso_y_pred[[i]]), type = "2")
# Matriz utilizada para los modelos Stepwise Forward y Backward Regression
mat_x_step <- as.data.frame(cbind(y, x1, x2, x3, x4, x5))
# Stepwise Forward Regression
fwd_est[[i]] <- stepwise_f(y, x1, x2, x3, x4, x5, "forward")
fwd_beta_error[i] <- norm(as.vector(unlist(betas_real_com) -
unlist(beta_error_f(fwd_est[[i]]))),
type = "2")
fwd_y_pred[[i]] <- predict(fwd_est[[i]], newdata = mat_x_step, 1)
fwd_y_error[i] <- norm(as.vector(y - fwd_y_pred[[i]]), type = "2")
fwd_sel[i] <- stepwise_sel_f(fwd_est[[i]])
# Stepwise Backward Regression
bwd_est[[i]] <- stepwise_f(y, x1, x2, x3, x4, x5, "backward")
bwd_beta_error[i] <- norm(as.vector(unlist(betas_real_com) -
unlist(beta_error_f(bwd_est[[i]]))),
type = "2")
bwd_y_pred[[i]] <- predict(bwd_est[[i]], newdata = mat_x_step, 1)
bwd_y_error[i] <- norm(as.vector(y - bwd_y_pred[[i]]), type = "2")
bwd_sel[i] <- stepwise_sel_f(bwd_est[[i]])
}
## Resultados finales
# OLS
mean(ols_beta_error)
mean(ols_y_error)
# LAD
mean(lad_beta_error)
mean(lad_y_error)
# Chebyshev
mean(cheb_beta_error)
mean(cheb_y_error)
# Ridge
mean(ridge_beta_error)
mean(ridge_y_error)
# Lasso
mean(lasso_beta_error)
mean(lasso_y_error)
# Forward
mean(fwd_beta_error)
mean(fwd_y_error)
# Backward
mean(bwd_beta_error)
mean(bwd_y_error)
mean(ols_sel)
mean(fwd_sel)
mean(bwd_sel)
## Examen v2: Utilizando IBEX
# Revisar si las librerias requeridas existen e instalar las que hagan falta
list_packages <- c("Quandl", "quantmod", "forecast", "tseries", "pander",
"memisc")
new_packages <- list_packages[!(list_packages %in%
rownames(installed.packages()))]
if (length(new_packages) > 0) install.packages(new_packages)
# Cargar librerias requeridas
library(Quandl)
library(quantmod)
library(forecast)
library(tseries)
library(pander)
library(memisc)
# Obtener datos de las cotizaciones del Brent
brent_ts <- Quandl(code = "FRED/DCOILBRENTEU",
type = "xts",
collapse = "weekly",
start_date = "2002-01-13",
end_date = "2016-05-29")
# Grafico de la cotizacion del barril Brent
plot(brent_ts, main = "Figura 1: Cotización del barril Brent")
# Transformacion Box-Cox
lambda_opt <- BoxCox.lambda(brent_ts)
brent_bc <- BoxCox(brent_ts, lambda = lambda_opt)
# Correlogramas de la serie Brent
tsdisplay(brent_bc, main = "Figura 2: Correlogramas de la serie Brent")
# Tests KPSS y ADF a nivel
kpss.test(brent_bc)
adf.test(brent_bc)
# Correlogramas de la serie Brent diferenciada
tsdisplay(diff(brent_bc), main = "Figura 3: Correlogramas de la serie D(Brent)")
# Tests KPSS y ADF para 1 diferencia
kpss.test(diff(brent_bc)[-1, ])
adf.test(diff(brent_ts)[-1,])
# Modelos ARIMA, estimados
arima_110 <- Arima(brent_ts, order = c(1, 1, 0), lambda = lambda_opt)
arima_011 <- Arima(brent_ts, order = c(0, 1, 1), lambda = lambda_opt)
arima_111 <- Arima(brent_ts, order = c(1, 1, 1), lambda = lambda_opt)
arima_211 <- Arima(brent_ts, order = c(2, 1, 1), lambda = lambda_opt)
arima_112 <- Arima(brent_ts, order = c(1, 1, 2), lambda = lambda_opt)
arima_212 <- Arima(brent_ts, order = c(2, 1, 2), lambda = lambda_opt)
# Evaluacion accuracy (frente a benchmarks)
acc_arima <- c()
acc_naive <- c()
acc_naivedr <- c()
for (j in 1:6) {
mae_arima <- c()
mae_naive <- c()
mae_naivedr <- c()
fcast_arima <- c()
fcast_naive <- c()
fcast_naivedr <- c()
h <- j * 2
opLambda <- BoxCox.lambda(brent_ts)
for (i in seq(4, 750 - h, 1)) {
xshort <- window(brent_ts, start = time(brent_ts)[1],
end = time(brent_ts)[i])
xpred <- window(brent_ts, start = time(brent_ts)[i + 1],
end = time(brent_ts)[i + h])
fit_arima <- Arima(xshort, model = arima_111)
fcast_arima <- forecast(fit_arima, h = h)
fcast_naive <- rwf(coredata(xshort), h = h)
fcast_naivedr <- rwf(coredata(xshort), h = h, drift = TRUE)
mae_arima[i] <- accuracy(fcast_arima, xpred)[2, "MAE"]
mae_naive[i] <- accuracy(fcast_naive, xpred)[2, "MAE"]
mae_naivedr[i] <- accuracy(fcast_naivedr, xpred)[2, "MAE"]
}
acc_arima[j] <- mean(mae_arima, na.rm = TRUE)
acc_naive[j] <- mean(mae_naive, na.rm = TRUE)
acc_naivedr[j] <- mean(mae_naivedr, na.rm = TRUE)
}
df <- data.frame(Horizonte = seq(2, 12, 2), acc_arima, acc_naive, acc_naivedr)
plot(df$Horizonte, df$acc_arima, type = "l", col = 2,
xlab = "Horizonte temporal",
ylab = "RMSE",
main = "Figura 6: ARIMA vs Benchmarks",
lwd = 1.5)
lines(df$Horizonte, df$acc_naive, col = 3, lwd = 1.5)
lines(df$Horizonte, df$acc_naivedr, col = "blue", lwd = 1.5)
legend("topleft", legend = c("ARIMA", "Naïve", "Naïve + Deriva"),
fill = c(2, 3, "blue"))
# Backtesting del modelo para ver porcentaje de cobertura
fcast_arima <- c()
fcast_naive <- c()
fcast_naivedr <- c()
int_80 <- c()
int_95 <- c()
totalpred <- c()
h <- 2
opLambda <- BoxCox.lambda(brent_ts)
for (i in seq(4, 750 - h, 1)) {
xshort <- window(brent_ts, start = time(brent_ts)[1],
end = time(brent_ts)[i])
xpred <- window(brent_ts, start = time(brent_ts)[i + 1],
end = time(brent_ts)[i + h])
fit_arima <- Arima(xshort, model = arima_111)
fcast_arima <- forecast(fit_arima, h = h)
totalpred <- sum(totalpred, h)
# Evaluación para intervalo de 80%
int_80 <- sum(int_80, sum(fcast_arima$lower[1] < as.vector(xpred) &
fcast_arima$upper[1] > as.vector(xpred)))
# Evaluación para intervalo de 95%
int_95 <- sum(int_95, sum(fcast_arima$lower[2] < as.vector(xpred) &
fcast_arima$upper[2] > as.vector(xpred)))
}
int_80/totalpred
int_95/totalpred
a <- 40
c <- 40
ec_1 <- (a/2) * (a/80) + 40 * (1 - a/80)
ec_2
ec_1
a <- 40
c <- 30
ec_1 <- (a/2) * (a/80) + 40 * (1 - a/80)
ec_2 <- (a/2 + c/2) * (a/80) * (c/80) +
(c/2 + 40) * (1 - a/80) * (c/80) +
(40 + 40) * (1 - c/80)
ec_1
ec_2
a <- seq(0, 79, 0.01)
c <- seq(0, 79, 0.01)
# 3 signals, 2 wands function
exp_t <- function(a, c){
(a/2 + c/2) * (a/80) * (c/80) +
(c/2 + 40) * (1 - a/80) * (c/80) +
(40 + 40) * (1 - c/80)
}
optim(par = c(10, 20), fn = test, c = c)
a <- seq(0, 79, 0.01)
c <- seq(0, 79, 0.01)
ec_1 <- (a/2) * (a/80) + 40 * (1 - a/80)
ec_2 <- (a/2 + c/2) * (a/80) * (c/80) +
(c/2 + 40) * (1 - a/80) * (c/80) +
(40 + 40) * (1 - c/80)
min(ec_2)
which.min(ec_2)
a[which.min(ec_2)]
c[which.min(ec_2)]
min(ec_1)
which.min(ec_1)
a[which.min(ec_1)]
c[which.min(ec_2)]
waiting_time <- function(s, w, n = 10000, alpha = 0.05) {
stopifnot(s >= 1 && w >= 0 && n >=1 && alpha>=0.00001 && alpha<=0.99999)
if (w >= s) {
return(0)
} else if (w == 0) {
return(s * 40)
} else {
# Create vectors of uniforms (0, 80)
mat_unif <- matrix(runif(n * s, 0, 80), nrow = n, ncol = s)
# Create vectors of cumulative probabilities
mat_cumprob <- matrix(data = mat_unif[, -s]/80, nrow = n, ncol = s - 1,
byrow = FALSE)
# Create vectors of comparisons
mat_comps <- matrix(0, nrow = n, ncol = s - 1)
for (i in seq(s - 1, 1, -1)) {
mat_comps[, s - i] <- sapply(mat_cumprob[, s - i],
function (x) (1 - x > x ^ i))
}
# View(mat_comps)
# Create matrix of conditions evaluation
mat_conds <- matrix(0, nrow = n, ncol = s)
mat_conds[, 1] <- mat_comps[, 1]
if (s - w > 1) {
for (i in seq(2, s - 1, 1)) {
past_comps <- apply(mat_conds, 1, sum)
rem_wands <- (i - 1) - past_comps
# print(head(mat_conds))
# print(head(rem_wands))
# print(head(w - rem_wands))
mat_conds[, i] <-  rem_wands < i & (
(past_comps < i - w) | (past_comps == i - w & mat_comps[, i]))
}
}
mat_conds[, s] <- apply(mat_conds, 1, sum) < s - w
# View(mat_conds)
# Bind both matrixes by columns (ignoring the last cumulative prob)
mat_unif_f <- cbind(mat_unif, mat_cumprob, mat_conds)
mat_wtime <- mat_unif_f[, 1:s] *
mat_unif_f[, seq(dim(mat_unif_f)[2] - s + 1,
dim(mat_unif_f)[2], 1)]
mat_unif_f <- cbind(mat_unif_f, mat_wtime, apply(mat_wtime, 1, sum))
ret_msg <- NA
ret_msg[1] <- "Mean: "
ret_msg[2] <- round(mean(mat_unif_f[, dim(mat_unif_f)[2]]), 3)
ret_msg[3] <- " | Confidence interval: ("
ret_msg[4] <- as.numeric(ret_msg[2]) -
round(qnorm(1 - alpha/2) *
sd(mat_unif_f[, dim(mat_unif_f)[2]])/sqrt(n), 3)
ret_msg[5] <- ", "
ret_msg[6] <- as.numeric(ret_msg[2]) +
round(qnorm(1 - alpha/2) *
sd(mat_unif_f[, dim(mat_unif_f)[2]])/sqrt(n), 3)
ret_msg[7] <- ")"
ret_msg <- paste0(ret_msg, collapse = "")
return(ret_msg)
}
}
waiting_time(3,1)
waiting_time(3, 1)
waiting_time(3, 1, n = 10000)
a <- seq(0, 79, 0.01)
c <- seq(0, 79, 0.01)
ec_2 <- (a/2 + c/2) * (a/80) * (c/80) +
(c/2 + 40) * (1 - a/80) * (c/80) +
(40 + 40) * (1 - c/80)
min(ec_2)
which.min(ec_2)
a[which.min(ec_2)]
c[which.min(ec_2)]
ec_2 <- (a/2 + c/2) * (a/80) * (c/80) +
(c/2 + 40) * (1 - a/80) * (c/80) +
(40 + 40) * (1 - c/80)
min(ec_2)
which.min(ec_2)
a[which.min(ec_2)]
c[which.min(ec_2)]
ec_2 <- (a/2 + c/2) * (a/80) * (c/80) +
(c/2 + 40) * (1 - a/80) * (c/80) +
(40 + 40) * (1 - c/80) * (a/80)
min(ec_2)
which.min(ec_2)
a[which.min(ec_2)]
c[which.min(ec_2)]
ec_2 <- (a/2 + c/2) * (a/80) * (c/80) +
(c/2 + 40) * (1 - a/80) * (c/80) +
(40 + 40) * (1 - c/80) * (1 - a/80)
min(ec_2)
which.min(ec_2)
a[which.min(ec_2)]
c[which.min(ec_2)]
k1 <- seq(0, 79, 0.01)
k2 <- seq(0, 79, 0.01)
ec_1 <- (a/2) * (a/80) + 40 * (1 - a/80)
ec_2 <- (k1/2 + k2/2) * (k1/80) * (k2/80) +
(k1/2 + 40) * (k1/80) * (1 - k2/80)  +
(40 + 40) * (1 - k1/80)
k1[which.min(ec_2)]
k2[which.min(ec_2)]
ec_2 <- (k1/2 + k2/2) * (k1/80) * (k2/80) +
(k1/2 + 40) * (k1/80) * (1 - k2/80)  +
(40 + 40) * (1 - k1/80) * 0.5
which.min(ec_2)
k1[which.min(ec_2)]
k2[which.min(ec_2)]
x11()
k1 <- seq(0, 79, 0.01)
k2 <- seq(0, 79, 0.01)
ec_1 <- (a/2) * (a/80) + 40 * (1 - a/80)
ec_2 <- (k1/2 + k2/2) * (k1/80) * (k2/80) +
(k1/2 + 40) * (k1/80) * (1 - k2/80)  +
(40 + 40) * (1 - k1/80)
persp(k1, k2, ec_2, theta=-30,phi=15,ticktype="detailed")
persp
z
ec_2
k1
k2
plot(k1, ec_2)
plot(k2, ec_2)
k1 <- seq(0, 79, 0.01)
k2 <- seq(0, 79, 0.01)
ec_1 <- (a/2) * (a/80) + 40 * (1 - a/80)
ec_2 <- (k1/2 + k2/2) * (k1/80) * (k2/80) +
(k1/2 + 40) * (k1/80) * (1 - k2/80)  +
(40 + 40) * (1 - k1/80) * (k2/80)
min(ec_2)
which.min(ec_2)
k1[which.min(ec_2)]
k2[which.min(ec_2)]
k2 <- 40
ec_2 <- (k1/2 + k2/2) * (k1/80) * (k2/80) +
(k1/2 + 40) * (k1/80) * (1 - k2/80)  +
(40 + 40) * (1 - k1/80)
min(ec_2)
which.min(ec_2)
k1[which.min(ec_2)]
k2[which.min(ec_2)]
min(ec_2)
which.min(ec_2)
k1[which.min(ec_2)]
k2[which.min(ec_2)]
data_str
library(data.table)
library(dplyr)
library(ggplot2)
library(ggthemes)
data_str <- fread("stroopdata.csv")
data_str <- data_str %>% mutate(Diff = Incongruent - Congruent)
setwd("~/Dropbox/Udacity/DAND/P1")
library(data.table)
library(dplyr)
library(ggplot2)
library(ggthemes)
data_str <- fread("stroopdata.csv")
data_str <- data_str %>% mutate(Diff = Incongruent - Congruent)
data_str
data_str$Diff < 10
sum(data_str$Diff < 10)
sum(data_str$Diff < 10)/24
sum(data_str$Diff < 12)/24
sum(data_str$Diff < 11)/24
sum(data_str$Diff < 12)/24
sum(data_str$Diff < 10)/24
sum(data_str$Diff < 7.9)/24
dim(data_str)
length(data_str)
ncol(data_str)
nrow(data_str)
sem <- sd_str/sqrt(nrow(data_str))
qt(0.95)
t.test(t_crit, alternative = "greater", mu = 0, conf.level = 0.95)
mean_str <- round(mean(data_str$Diff), 2)
med_str <- round(median(data_str$Diff), 2)
sd_str <- round(sd(data_str$Diff), 2)
max_str <- round(max(data_str$Diff), 2)
min_str <- round(min(data_str$Diff), 2)
n_str <- nrow(data_str)
df <- n_str - 1
sem <- sd_str/sqrt(n_str)
t_stat <- (mean_str - 0) / sem
t_crit <- qt(0.95, df)
marg_err <- t_crit * sem
cohen_d <- mean_str/sd_str
rsq <- (t_stat^2)/(t_stat^2 + df)
t.test(t_crit, alternative = "greater", mu = 0, conf.level = 0.95)
t_crit
t.test(data_str, alternative = "greater", mu = 0, conf.level = 0.95)
t.test(data_str$Diff, alternative = "greater", mu = 0, conf.level = 0.95)
t_stat <- (mean_str - 0) / sem
t_stat
t_crit <- qt(0.95, df)
t_crit
marg_err <- t_crit * sem
marg_err
mean_str + marg_err
mean_str - marg_err
rsq <- (t_stat^2)/(t_stat^2 + df)
rsq
cohen_d <- mean_str/sd_str
cohen_d
t_stat > t_crit
t_stat
